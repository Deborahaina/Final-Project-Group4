{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, hamming_loss, cohen_kappa_score, matthews_corrcoef\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils import data\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OR_PATH = os.getcwd()\n",
    "os.chdir(\"..\") \n",
    "\n",
    "\n",
    "PATH = os.getcwd()\n",
    "DATA_DIR = os.getcwd() + os.path.sep + 'train' + os.path.sep\n",
    "sep = os.path.sep\n",
    "\n",
    "os.chdir(OR_PATH) # Come back to the directory where the code resImageIdes \n",
    "\n",
    "\n",
    "#Inception net must have image sizes as 299 * 299\n",
    "n_epoch = 2\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "\n",
    "## Image processing\n",
    "CHANNELS = 3\n",
    "IMAGE_SIZE = 299\n",
    "\n",
    "NICKNAME = \"Group4\"\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "THRESHOLD = 0.5\n",
    "SAVE_MODEL = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(PATH+os.path.sep + \"excel\"):\n",
    "        if file[-5:] == '.xlsx':\n",
    "            FILE_NAME = PATH+ os.path.sep+ \"excel\" + os.path.sep + file\n",
    "            print(FILE_NAME)\n",
    "\n",
    "        # Reading and filtering Excel file\n",
    "        xdf_data = pd.read_excel(FILE_NAME)\n",
    "        \n",
    "        xdf_dset = xdf_data[xdf_data[\"Split\"] == \"train\"].copy()\n",
    "    \n",
    "        xdf_dset_test = xdf_data[xdf_data[\"Split\"] == \"test\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Custom data class to read in the images one sample at a time using the image ImageId\n",
    "\n",
    "    Processing the multilabel target(catgeory) into one-hot-encoding\n",
    "    \n",
    "    Returns the transformed images and labels\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class ImageDataset(data.Dataset):\n",
    "    def __init__(self, list_IDs, type_data, target_type):\n",
    "        #Initialization'\n",
    "        self.type_data = type_data\n",
    "        self.list_IDs = list_IDs\n",
    "        self.target_type = target_type\n",
    "        \n",
    "        self.transforms = v2.Compose([\n",
    "            transforms.Resize((299, 299)),\n",
    "            v2.CenterCrop(299),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0),\n",
    "            v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "    #Denotes the total number of samples'  \n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ID = self.list_IDs[index]\n",
    "        \n",
    "        #Genrate one sample at a time by using loading the image file and the id\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "\n",
    "        if self.type_data == 'train':\n",
    "            y = xdf_dset.target_class.get(ID)\n",
    "            if self.target_type == 2:\n",
    "                y = y.split(\",\")\n",
    "        else:\n",
    "            y = xdf_dset_test.target_class.get(ID)\n",
    "            if self.target_type == 2:\n",
    "                y = y.split(\",\")\n",
    "\n",
    "\n",
    "        if self.target_type == 2:\n",
    "            labels_ohe = [ int(e) for e in y]\n",
    "        else:\n",
    "            labels_ohe = np.zeros(OUTPUTS_a)\n",
    "\n",
    "            for idx, label in enumerate(range(OUTPUTS_a)):\n",
    "                if label == y:\n",
    "                    labels_ohe[idx] = 1\n",
    "\n",
    "        y = torch.FloatTensor(labels_ohe)\n",
    "\n",
    "        if self.type_data == 'train':\n",
    "            file = DATA_DIR + xdf_dset.ImageId.get(ID)\n",
    "        else:\n",
    "            file = DATA_DIR + xdf_dset_test.ImageId.get(ID)\n",
    "\n",
    "        img = cv2.imread(file)\n",
    "\n",
    "        img= cv2.resize(img,(IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "        # Augmentation for training\n",
    "        X = torch.FloatTensor(img)\n",
    "\n",
    "        X = torch.reshape(X, (3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "        if self.type_data == 'train':\n",
    "            X = self.transforms(X)\n",
    "\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom model\n",
    "class FashionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, (3, 3))\n",
    "        self.convnorm1 = nn.BatchNorm2d(16)\n",
    "        self.pad1 = nn.ZeroPad2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 128, (3, 3))\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 100, (3, 3))\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.linear = nn.Linear(100, OUTPUTS_a)\n",
    "        self.act = torch.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad1(self.convnorm1(self.act(self.conv1(x))))\n",
    "        x = self.act(self.conv2(self.act(x)))\n",
    "        return self.linear(self.global_avg_pool(x).view(-1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_target(target_type):\n",
    "    '''\n",
    "        1- Binary   target = (1,0)\n",
    "        2- Multiclass  target = (1...n, text1...textn)\n",
    "        3- Multilabel target = ( list(Text1, Text2, Text3 ) for each observation, separated by commas )\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    dict_target = {}\n",
    "    xerror = 0\n",
    "\n",
    "    if target_type == 2:\n",
    "        ## The target comes as a string  x1, x2, x3,x4\n",
    "        ## the following code creates a list\n",
    "        target = np.array(xdf_data['Category'].apply( lambda x : x.split(\",\")))\n",
    "        final_target = mlb.fit_transform(target)\n",
    "        xfinal = []\n",
    "        if len(final_target) ==0:\n",
    "            xerror = 'Could not process Multilabel'\n",
    "        else:\n",
    "            class_names = mlb.classes_\n",
    "            for i in range(len(final_target)):\n",
    "                joined_string = \",\".join( str(e) for e in final_target[i])\n",
    "                xfinal.append(joined_string)\n",
    "            xdf_data['target_class'] = xfinal\n",
    "\n",
    "    if target_type == 1:\n",
    "        xtarget = list(np.array(xdf_data['Category'].unique()))\n",
    "        le = LabelEncoder()\n",
    "        le.fit(xtarget)\n",
    "        final_target = le.transform(np.array(xdf_data['Category']))\n",
    "        class_names=(xtarget)\n",
    "        xdf_data['target_class'] = final_target\n",
    "\n",
    "    ## We add the column to the main dataset\n",
    "\n",
    "\n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(target_type):\n",
    "    ## Only the training set\n",
    "    ## xdf_dset ( data set )\n",
    "    ## read the data data from the file\n",
    "\n",
    "\n",
    "    ds_inputs = np.array(DATA_DIR + xdf_dset['ImageId'])\n",
    "\n",
    "    ds_targets = xdf_dset['target_class']\n",
    "\n",
    "    # ---------------------- Parameters for the data loader --------------------------------\n",
    "\n",
    "    list_of_ids = list(xdf_dset.index)\n",
    "    list_of_ids_test = list(xdf_dset_test.index)\n",
    "\n",
    "\n",
    "    # Datasets\n",
    "    partition = {\n",
    "        'train': list_of_ids,\n",
    "        'test' : list_of_ids_test\n",
    "    }\n",
    "\n",
    "    # Data Loaders\n",
    "\n",
    "    params = {'batch_size': BATCH_SIZE,\n",
    "              'shuffle': True, 'num_workers':4}\n",
    "\n",
    "    training_set = ImageDataset(partition['train'], 'train', target_type)\n",
    "    training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "    params = {'batch_size': BATCH_SIZE,\n",
    "              'shuffle': False}\n",
    "\n",
    "    test_set = ImageDataset(partition['test'], 'test', target_type)\n",
    "    test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "    ## Make the channel as a list to make it variable\n",
    "\n",
    "    return training_generator, test_generator\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    # Open the file\n",
    "\n",
    "    print(model, file=open('summary_inception{}.txt'.format(NICKNAME), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "alpha: Scale the focal weight with alpha.\n",
    "\n",
    "gamma: Take the power of the focal weight with gamma.\n",
    "\n",
    "Returns focal weight * Binary binary_cross_entropy\n",
    "\n",
    "#The closer to zero gamma is, the more confident in the class predicted. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=True, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        predicted_target = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-predicted_target)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "        \n",
    "\n",
    "def model_definition(pretrained=False):\n",
    "    # Define a Keras sequential model\n",
    "    # Compile the model\n",
    "\n",
    "    if pretrained == True:\n",
    "        model = models.inception_v3()\n",
    "\n",
    "        model.fc = nn.Linear(model.fc.in_features, OUTPUTS_a)\n",
    "    else:\n",
    "        model = FashinonNet()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    criterion= FocalLoss()\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=0, verbose=True)\n",
    "\n",
    "    save_model(model)\n",
    "\n",
    "    return model, optimizer, criterion, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_func(metrics, aggregates, y_true, y_pred):\n",
    "    '''\n",
    "    multiple functiosn of metrics to call each function\n",
    "    f1, cohen, accuracy, mattews correlation\n",
    "    list of metrics: f1_micro, f1_macro, f1_avg, coh, acc, mat\n",
    "    list of aggregates : avg, sum\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    def f1_score_metric(y_true, y_pred, type):\n",
    "        '''\n",
    "        type = micro,macro,weighted,samples\n",
    "        :param y_true:\n",
    "        :param y_pred:\n",
    "        :param average:\n",
    "        :return: res\n",
    "        '''\n",
    "        res = f1_score(y_true, y_pred, average=type)\n",
    "        return res\n",
    "\n",
    "    def cohen_kappa_metric(y_true, y_pred):\n",
    "        res = cohen_kappa_score(y_true, y_pred)\n",
    "        return res\n",
    "\n",
    "    def accuracy_metric(y_true, y_pred):\n",
    "        res = accuracy_score(y_true, y_pred)\n",
    "        return res\n",
    "\n",
    "    xcont = 1\n",
    "    xsum = 0\n",
    "    xavg = 0\n",
    "    res_dict = {}\n",
    "    \n",
    "    for xm in metrics:\n",
    "        if xm == 'f1_micro':\n",
    "            # f1 score average = micro\n",
    "            xmet = f1_score_metric(y_true, y_pred, 'micro')\n",
    "        elif xm == 'f1_macro':\n",
    "            # f1 score average = macro\n",
    "            xmet = f1_score_metric(y_true, y_pred, 'macro')\n",
    "        elif xm == 'f1_weighted':\n",
    "            # f1 score average =\n",
    "            xmet = f1_score_metric(y_true, y_pred, 'weighted')\n",
    "        elif xm == 'coh':\n",
    "             # Cohen kappa\n",
    "            xmet = cohen_kappa_metric(y_true, y_pred)\n",
    "        elif xm == 'acc':\n",
    "            # Accuracy\n",
    "            xmet = accuracy_metric(y_true, y_pred)\n",
    "        else:\n",
    "            xmet = 0\n",
    "\n",
    "        res_dict[xm] = xmet\n",
    "\n",
    "        xsum = xsum + xmet\n",
    "        xcont = xcont +1\n",
    "\n",
    "    if 'sum' in aggregates:\n",
    "        res_dict['sum'] = xsum\n",
    "    if 'avg' in aggregates and xcont > 0:\n",
    "        res_dict['avg'] = xsum/xcont\n",
    "    # Ask for arguments for each metric\n",
    "\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_test(train_ds, test_ds, list_of_metrics, list_of_agg, save_on, pretrained=False):\n",
    "    \n",
    "    class_names = process_target(2)\n",
    "    OUTPUTS_a = len(class_names)\n",
    "    # list_of_metrics, list_of_agg, save_on='f1_macro',\n",
    "    \n",
    "    # Define the model, optimizer, criterion, scheduler, and other necessary components\n",
    "    model, optimizer, criterion, scheduler = model_definition(pretrained=pretrained)\n",
    "    \n",
    "    # logits = inception_outputs.logits, add this for inceptionnet to work and loss = F.binary_cross_entropy_with_logits(logits, target)\n",
    "\n",
    "    cont = 0\n",
    "    \n",
    "    #Save the training and testing loss\n",
    "    train_loss_item = []\n",
    "    test_loss_item = []\n",
    "\n",
    "    #Track the best test metric\n",
    "    met_test_best = 0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, steps_train = 0, 0\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        pred_labels_list = []\n",
    "        \n",
    "        pred_labels_list_test = []\n",
    "        \n",
    "        pred_logits, real_labels = np.zeros((1, OUTPUTS_a)), np.zeros((1, OUTPUTS_a))\n",
    "        \n",
    "        \n",
    "        train_hist = list([])\n",
    "\n",
    "        # Iterate through the training dataset\n",
    "        with tqdm(total=len(train_ds), desc=\"Epoch {}\".format(epoch)) as pbar:\n",
    "            \n",
    "            #forward pass\n",
    "            for xdata, xtarget in train_ds:\n",
    "                xdata, xtarget = xdata.to(device), xtarget.to(device)        \n",
    "                optimizer.zero_grad()\n",
    "                output = model(xdata)\n",
    "                \n",
    "                #error\n",
    "                loss = criterion(output.logits, xtarget)\n",
    "                \n",
    "                #backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                cont +=1\n",
    "                steps_train += 1\n",
    "\n",
    "                # Detach the tensor and move it to CPU before converting to numpy\n",
    "                pred_labels_per = output.logits.cpu().detach().numpy()\n",
    "                \n",
    "                #Append the predicted labels to the prediction labels list\n",
    "                if len(pred_labels_list) == 0:\n",
    "                    pred_labels_list = pred_labels_per\n",
    "                else:\n",
    "                    pred_labels_list = np.vstack([pred_labels_list, pred_labels_per])  \n",
    "                    \n",
    "                if len(train_hist) == 0:\n",
    "                    train_hist = xtarget.cpu().numpy()\n",
    "                else:\n",
    "                    train_hist = np.vstack([train_hist, xtarget.cpu().numpy()])\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix_str(\"Train Loss: {:.5f}\".format(train_loss / steps_train))\n",
    "                \n",
    "                pred_logits = np.vstack((pred_logits, output.logits.detach().cpu().numpy()))\n",
    "                real_labels = np.vstack((real_labels, xtarget.cpu().numpy()))\n",
    "\n",
    "        pred_labels = pred_logits[1:]\n",
    "        pred_labels[pred_labels >= THRESHOLD] = 1\n",
    "        pred_labels[pred_labels < THRESHOLD] = 0\n",
    "\n",
    "        #Metric Evaluation\n",
    "        train_metrics = metrics_func(list_of_metrics, list_of_agg, real_labels[1:], pred_labels)\n",
    "        # train_loss /= len(train_ds)\n",
    "        \n",
    "           \n",
    "\n",
    "        # Evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        \n",
    "        #Iterate through the testing dataset\n",
    "        test_loss, steps_test = 0, 0\n",
    "        met_test = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            with tqdm(total=len(test_ds), desc=\"Epoch {}\".format(epoch)) as pbar:\n",
    "                \n",
    "                for xdata, xtarget in test_ds:\n",
    "                    \n",
    "                    xdata, xtarget = xdata.to(device), xtarget.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(xdata)\n",
    "                    loss = criterion(output, xtarget)\n",
    "                    test_loss += loss.item()\n",
    "                    \n",
    "                    steps_test += 1\n",
    "                    cont +=1\n",
    "\n",
    "                    # Detach the tensor and move it to CPU before converting to numpy\n",
    "                    pred_labels_per = output.cpu().detach().numpy()\n",
    "                    \n",
    "                    #Append the predicted labels to the prediction labels list\n",
    "                    if len(pred_labels_list_test) == 0:\n",
    "                        pred_labels_list_test = pred_labels_per\n",
    "                    else:\n",
    "                        pred_labels_list_test = np.vstack([pred_labels_list_test, pred_labels_per]) \n",
    "                        \n",
    "                    if len(test_hist) == 0:\n",
    "                        test_hist = xtarget.cpu().numpy()\n",
    "                    else: \n",
    "                        test_hist = np.vstack([test_hist, xtarget.cpu().numpy()])\n",
    "\n",
    "                    #Update progress bar\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix_str(\"Test Loss: {:.5f}\".format(test_loss / steps_test))\n",
    "                    \n",
    "                    pred_logits_test = np.vstack((pred_logits, output.detach().cpu().numpy()))\n",
    "                    real_labels = np.vstack((real_labels, xtarget.cpu().numpy()))\n",
    "                    \n",
    "        # Extract true labels for testing dataset\n",
    "        pred_labels_test = pred_logits_test[1:]\n",
    "        pred_labels_test[pred_labels_test >= THRESHOLD] = 1\n",
    "        pred_labels_test[pred_labels_test < THRESHOLD] = 0\n",
    "        \n",
    "        # Metric Evaluation\n",
    "        test_metrics = metrics_func(list_of_metrics, list_of_agg, real_labels[1:], pred_labels_test)\n",
    "        acc_test = accuracy_score(real_labels[1:], pred_labels_test)\n",
    "        print(acc_test)\n",
    "\n",
    "\n",
    "        #hml_test = hamming_loss(real_labels[1:], pred_labels)\n",
    "\n",
    "        avg_test_loss = test_loss / steps_test\n",
    "\n",
    "        xstrres = \"Epoch {}: \".format(epoch)\n",
    "        for met, dat in train_metrics.items():\n",
    "            xstrres = xstrres +' Train '+met+ ' {:.5f}'.format(dat)\n",
    "\n",
    "\n",
    "        xstrres = xstrres + \" - \"\n",
    "        for met, dat in test_metrics.items():\n",
    "            xstrres = xstrres + ' Test '+met+ ' {:.5f}'.format(dat)\n",
    "            if met == save_on:\n",
    "                met_test = dat\n",
    "\n",
    "        print(xstrres)\n",
    "\n",
    "        if met_test > met_test_best and SAVE_MODEL:\n",
    "\n",
    "            torch.save(model.state_dict(), \"model_inception{}.pt\".format(NICKNAME))\n",
    "            xdf_dset_results = xdf_dset_test.copy()\n",
    "            \n",
    "            met_test_best = met_test\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    ## Target_type = 1  Multiclass   Target_type = 2 MultiLabel\n",
    "    class_names = process_target(target_type = 2)\n",
    "    print(class_names)\n",
    "    \n",
    "    ## read_data creates the dataloaders\n",
    "    train_ds,test_ds = read_data(target_type = 2)\n",
    "\n",
    "    OUTPUTS_a = len(class_names)\n",
    "\n",
    "    list_of_metrics = ['f1_macro']\n",
    "    list_of_agg = ['avg']\n",
    "    \n",
    "    \n",
    "\n",
    "    train_and_test(train_ds, test_ds, list_of_metrics, list_of_agg, save_on='f1_macro', pretrained=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
